

...............................
--> WORKER NODE

EFS 
  Create EFS filesystem attach it to worker node at specific path
  
...............................
--> JENKINS SERVER EC2

Instance type : c5large  Family: [......aws linux......]
  software or tools install  
    1) git 
    2) docker 
      yum install docker -y 
      systemctl start docker 
      systemctl enable docker 
    3) Jenkins

  login to dockerhub
      username:- masktv
      password:- mask@1234
    
  Packages :
    1) kubectl
      -  curl -LO "https://dl.k8s.io/release/v1.26.0/bin/linux/amd64/kubectl"
      -  chmod +x ./kubectl
      -  sudo mv ./kubectl /usr/local/bin/kubectl
      -  kubectl version --client

  b. Configure kubernetes user to perform kubernetes deployment task ----> if use role no need to configure user.
     - configure user with aws configure cammand [configure IAM User that have Access Entry Created in EKS cluster]

  c. Configure cluster on jenkins server
      using kubeconfig update cammand 
      -  aws eks --region <region> update-kubeconfig --name <cluster-name>

  d. Deploy ingress controller And ingress service

ATTACH EFS TO JENKINS EC2, AND SCHEDULE CRONJOB OF CMD TO ENTRE IN MASTER POD AND RUN MYSQLDUMP CMD TO TAKE BACKUP AND STORE IN EFS
...............................

--> JENKINES SERVER UI

Credential for jenkins pipeline
  github credential
  # dockerhub credential
  # kubernetes user credential

Necessory plugins on jenkines server  
  Pipeline stage view
  ssh build agent
  credential binding plugin
  credential plugin
  ssh agent plugin
  docker pipeline
  docker common plugn
  docker api plugin  
  git/github-plugin
  kubernetes-plugin
  docker plugin

For Automation project we will have to create 1 job and 1 or 2 pipeline.

  pipeline 1st :- kubernetes cluster configure, clone the repo from github, build the Dokcker image from db.dockerfile, docker push image on dockerhub, then deploy kubernetes secret.yaml, and deploy db.pv, db.pvc, db.deployment and db.service, db.hpa. 
  pipeline 2nd ;- kubernetes cluster configure, clone the repo from github, build the Dokcker image from app.dockerfile, docker push image on dockerhub, then deploy app.pv, app.pvc, app.deployment and app.service, ingress rule, app.hpa.

..................................
--> Cluster Creation 

Worker Node Role Permission.
  -  AmazonEBSCSIDriverPolicy
  -  AmazonEC2ContainerRegistryPullOnly
  -  AmazonEC2ContainerRegistryReadOnly
  -  AmazonEC2FullAccessAmazonEKS_CNI_Policy
  -  AmazonEKSWorkerNodePolicy
  -  AmazonElasticFileSystemFullAccess
  -  CloudWatchAgentServerPolicy
  -  CloudWatchLogsFullAccess

IAM User To Perform Deployment Action.
  -  AdministratorAccess

Access Entry to Access Cluster by IAM User To Perform Deployment Task.

  -  After creating cluster, Go to Access --> Create Access Entry --> Select IAM USER in IAM PRINCIPLE --> Next --> ADD AmazonEKSClusterAdminPolicy in Access policie --> Add Policy --> Next --> Review And Create Policy.

AddOns Need To install.

  -  CoreDNS, Amazon EBS CSI Driver, Amazon VPC CNI, Amazon EFS CSI Driver, kube-proxy, Metrics Server
.......................................................................................................

AUTOMATION OF HTML OR DEPLOYMENT VIA JENKINS
  -  Automation Process --> html
  -  make tar.gz of all file in html dir with cmd tar -czf file.tar.gz * [after extraction we should get file under html not html]
  -  every time to deploy file in production we will have need to save file with new name on s3 / or if we using same name need to delete pod
  -  after mention file name in download.yaml
  -  run kubectl apply -f download.yaml to deploy changes in production
..................................

 for dynamic provisioning, ebs-csi-driver not creating pvc and pv.
    ---> SOLVED 
    steps: -->
    1. add ebs-csi-driver at time create cluster.
    2. Modify instance metadata options
        choose WorkerNode ---> instance setting --> modify instance metadata --> chnage from REQUIRED to OPTIONAL
    3. after all changes performe go to server where cluster configure and run that command .... kubectl -n kube-system delete pod -l app=ebs-csi-controller
    4. check pod status and also check in volume section in ec2 volume section, there will have new volume created
